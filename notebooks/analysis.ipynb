{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e88d130e",
      "metadata": {},
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fff9eb0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/mila/m/marawan.gamal/scratch/ties/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/home/mila/m/marawan.gamal/scratch/ties/.venv/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T5Wrapper(\n",
            "  (transformer): T5ForConditionalGeneration(\n",
            "    (shared): Embedding(32128, 768)\n",
            "    (encoder): T5Stack(\n",
            "      (embed_tokens): Embedding(32128, 768)\n",
            "      (block): ModuleList(\n",
            "        (0): T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (relative_attention_bias): Embedding(32, 12)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseActDense(\n",
            "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): ReLU()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1-11): 11 x T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseActDense(\n",
            "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): ReLU()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (decoder): T5Stack(\n",
            "      (embed_tokens): Embedding(32128, 768)\n",
            "      (block): ModuleList(\n",
            "        (0): T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (relative_attention_bias): Embedding(32, 12)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerCrossAttention(\n",
            "              (EncDecAttention): T5Attention(\n",
            "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseActDense(\n",
            "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): ReLU()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1-11): 11 x T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerCrossAttention(\n",
            "              (EncDecAttention): T5Attention(\n",
            "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseActDense(\n",
            "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): ReLU()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 3/3 [00:17<00:00,  5.96s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from src.model.T5Wrapper import T5Wrapper\n",
        "\n",
        "# Init model\n",
        "MODEL_NAME = \"t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "transformer = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "model = T5Wrapper(transformer, tokenizer)\n",
        "print(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
